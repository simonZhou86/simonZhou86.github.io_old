<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.1">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lobster:300,300italic,400,400italic,700,700italic|Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"simonzhou86.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="Reinforcement Learning - ETC and UCB AlgorithmRecently, I started my first Research Project under the Department of Mathematics and Statistics at Queen’s University. The project mainly focus on the Mu">
<meta property="og:type" content="article">
<meta property="og:title" content="ETC Vs. UCB Algorithm">
<meta property="og:url" content="https://simonzhou86.github.io/2020/06/21/Reinforcement-Learning-Note-No-1/index.html">
<meta property="og:site_name" content="Simon&#39;s Blog">
<meta property="og:description" content="Reinforcement Learning - ETC and UCB AlgorithmRecently, I started my first Research Project under the Department of Mathematics and Statistics at Queen’s University. The project mainly focus on the Mu">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://simonzhou86.github.io/images/EE.jpg">
<meta property="og:image" content="https://simonzhou86.github.io/images/MAB.png">
<meta property="og:image" content="https://simonzhou86.github.io/images/code_experiment.png">
<meta property="article:published_time" content="2020-06-21T18:23:37.000Z">
<meta property="article:modified_time" content="2020-06-24T00:15:24.036Z">
<meta property="article:author" content="Simon Zhou">
<meta property="article:tag" content="Reinforcement Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://simonzhou86.github.io/images/EE.jpg">

<link rel="canonical" href="https://simonzhou86.github.io/2020/06/21/Reinforcement-Learning-Note-No-1/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>ETC Vs. UCB Algorithm | Simon's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Simon's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">Undergraduate Researcher/Data Science/Machine Learning</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
        <li class="menu-item menu-item-contact">

    <a href="/contact/" rel="section"><i class="far fa-address-book fa-fw"></i>contact</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>

  <a href="https://github.com/simonZhou86" class="github-corner" title="Follow me on GitHub" aria-label="Follow me on GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://simonzhou86.github.io/2020/06/21/Reinforcement-Learning-Note-No-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/profilepic.jpg">
      <meta itemprop="name" content="Simon Zhou">
      <meta itemprop="description" content="Update from time to time. Stay tuned!">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Simon's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          ETC Vs. UCB Algorithm
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-06-21 14:23:37" itemprop="dateCreated datePublished" datetime="2020-06-21T14:23:37-04:00">2020-06-21</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-23 20:15:24" itemprop="dateModified" datetime="2020-06-23T20:15:24-04:00">2020-06-23</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Research-Notes/" itemprop="url" rel="index"><span itemprop="name">Research Notes</span></a>
                </span>
            </span>

          
            <span id="/2020/06/21/Reinforcement-Learning-Note-No-1/" class="post-meta-item leancloud_visitors" data-flag-title="ETC Vs. UCB Algorithm" title="Views">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">Views: </span>
              <span class="leancloud-visitors-count"></span>
            </span>
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="far fa-comment"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/2020/06/21/Reinforcement-Learning-Note-No-1/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/2020/06/21/Reinforcement-Learning-Note-No-1/" itemprop="commentCount"></span>
    </a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="Reinforcement-Learning-ETC-and-UCB-Algorithm"><a href="#Reinforcement-Learning-ETC-and-UCB-Algorithm" class="headerlink" title="Reinforcement Learning - ETC and UCB Algorithm"></a>Reinforcement Learning - ETC and UCB Algorithm</h1><p>Recently, I started my first Research Project under the Department of Mathematics and Statistics at Queen’s University. The project mainly focus on the Multi-Armed Bandit Problems and Its solutions. This series of notes will focus on What I learned during this project(Algorithms, theory, etc.) and I will be also provide some proofs if necessary. </p>
<a id="more"></a>

<h2 id="Intuition-of-Exploration-Vs-Exploitation"><a href="#Intuition-of-Exploration-Vs-Exploitation" class="headerlink" title="Intuition of Exploration Vs. Exploitation"></a>Intuition of Exploration Vs. Exploitation</h2><p>The multi-armed bandit problem is a classical reinforcement learning topic, and it is also a good example to illustrate the trade off between exploration and exploitation. One classical example would be you are playing with the slot machines in casino, there are several of them and you do not know which one will give you the reward. So you decide to try each of them at first place, and then choose the one that give you the greatest reward in the previous round. Whether you choose to try different machines(explore), or you stick with one machine all the time(exploit); this is a perfect scenario that demonstrate the exploration and exploitation dilemma.</p>
<p>Exploration and Exploitation also applied to many other applications such as Clinic trails, recommendation systems and online advertising.</p>
<p><img src="/images/EE.jpg" alt=""></p>
<p>The above image is an example of exploration and exploitation of a slot machine.</p>
<p>If we know all the information about the environment(ex: which machine will give me the greatest money), we are always able to find the best solution, then there is no point to raise this dilemma. However, in real life, we are not able to know which machine will give you the greatest money. The dilemma comes strict from above situation, we have to gather enough information to make the best overall decision. With exploitation, we take the advantage of what we know; with exploration, we may need to take some risk to collect information from what we do not know. Our goal is to achieve the greatest amount of reward in the long-term.</p>
<p>In this post, we will only consider K-armed stochastic bandits(Multi-Armed) and K is a finite number.</p>
<h2 id="What-is-Multi-Armed-Bandit-K-Armed-Bandit"><a href="#What-is-Multi-Armed-Bandit-K-Armed-Bandit" class="headerlink" title="What is Multi-Armed Bandit (K-Armed Bandit)?"></a>What is Multi-Armed Bandit (K-Armed Bandit)?</h2><p>The Multi-Armed Bandit problem is a great explanation of exploration and exploitation dilemma. Imagine again you are in a casino, there are 5 slot machines in front of you, each machine have its own reward probability. Then the question becomes what is the best strategy that can help you achieve highest long-term rewards?</p>
<p><img src="/images/MAB.png" alt=""></p>
<p>This is an example of Multi-Armed Bandit problem, but before we dive into the strategies, we have to make some core assumptions. </p>
<h3 id="What-is-Stochastic-Bandit"><a href="#What-is-Stochastic-Bandit" class="headerlink" title="What is Stochastic Bandit:"></a>What is Stochastic Bandit:</h3><ol>
<li><p>A stochastic bandit is a collection of distributions denote by $ v = (P_a: a \in A) $, where A is a set of available actions.</p>
</li>
<li><p>The learner and the environment interact sequentially over n rounds. In each round $t \in {1….n}$, the learner choose an action $A_t \in A$, which is fed to the environment.</p>
</li>
<li><p>The environment samples a reward $X_t \in R$ from distribution $P_{A_t}$, and learner reveals $X_t$ rewards.</p>
</li>
<li><p>Usually the horizon n is finite, but sometimes allow the interaction to continue indefinitely (n = $\infty$)</p>
</li>
<li><p>The conditional distribution of reward $X_t$ given $A_1, X_1, …., A_{t-1}, X_{t-1}, A_t$ is $P_{A_t}$</p>
</li>
</ol>
<p>Notice that it is a simple [Markov Decision Process]: <a href="https://en.wikipedia.org/wiki/Markov_decision_process" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Markov_decision_process</a>, the distribution of reward $ X_t$ only depends on $A_t$.</p>
<h3 id="Environment-Classes"><a href="#Environment-Classes" class="headerlink" title="Environment Classes:"></a>Environment Classes:</h3><h4 id="Unstructured-Bandits"><a href="#Unstructured-Bandits" class="headerlink" title="Unstructured Bandits:"></a>Unstructured Bandits:</h4><p>An environment class $\epsilon$ is unstructured if $A$ is finite and there exist sets of distribution $M_a$ for each $a \in A$ such that:<br>$$<br>\epsilon = {v = (P_a: a \in A): P_a \in M_a \quad \forall a \in A}<br>$$</p>
<p><strong>For simplicity, under unstructured bandit environment, player action $ a $ the learner can not deduce anything about the distribution of actions for some actions $ b \neq a$</strong></p>
<h3 id="Structured-Bandits"><a href="#Structured-Bandits" class="headerlink" title="Structured Bandits:"></a>Structured Bandits:</h3><p>Environment classes that are not unstructured are called structured bandit. <strong>For simplicity, learner can often obtain information about some actions while never playing them.</strong> </p>
<h3 id="Regret-Analysis-Decomposition"><a href="#Regret-Analysis-Decomposition" class="headerlink" title="Regret Analysis/Decomposition:"></a>Regret Analysis/Decomposition:</h3><p>General Regret:<br>$$<br>R_n = n \mu^*(v) - E[\sum_{t=1}^{n} {X_t}]<br>$$</p>
<p>Where $\mu ^ * (v)$ is the largest mean of all arms</p>
<p>The Bayesian regret can be written as the following:<br>$$<br>BR_n(\pi, Q)= \int_{\varepsilon}{R_n(\pi, v)dQ(v)}<br>$$</p>
<p>Where Q is a prior probability measure on $ \varepsilon $. We will not talk about Bayesian regret for now.</p>
<p>Regret can also be decompose as the following:<br>$$<br>R_n = \sum_{a \in A}{\Delta_aE[T_a(n)]}<br>$$<br>Where:<br>$$<br>T_a(t) = \sum_{s=1}^{t}{\mathbb{I}[A_s = a]}<br>$$<br>is the number of times action a was chosen by the agent(learner) after end of round $ t $.<br>$ \Delta_a$ is the suboptimality gap between the rewards learner actual receives and the optimal rewards.</p>
<p>From the above formula, we know that we should aim to use an arm with a minimum suboptimality gap and try to avoid the one with larger gap.</p>
<p>Now we can finally step into the solutions for Multi-armed bandit problem.</p>
<h2 id="The-Explore-Then-Commit-Algorithm-for-Stochastic-Bandit-Problem-with-Finitely-many-Arms"><a href="#The-Explore-Then-Commit-Algorithm-for-Stochastic-Bandit-Problem-with-Finitely-many-Arms" class="headerlink" title="The Explore-Then-Commit Algorithm for Stochastic Bandit Problem with Finitely many Arms"></a>The Explore-Then-Commit Algorithm for Stochastic Bandit Problem with Finitely many Arms</h2><p>Note: In this and next algorithm, we assume that the reward distribution for all bandit instances are follow 1-sub-gaussian distribution for simplicity(The sub-gaussian constant is set to 1).</p>
<h3 id="ETC-Algorithm"><a href="#ETC-Algorithm" class="headerlink" title="ETC Algorithm:"></a>ETC Algorithm:</h3><p>Often when we do not know which arm perform the best, we then most likely will to try every arm. Imagine that you try every arm 50 times, and the first arm always give you the best, so for the remaining trails, you have a higher probability to stick with the first arm. That is the intuition of this ETC algorithm. The algorithm is characterized by the number of times it explores each arm which denote by $ m \in \mathbb{N}$. The algorithm will explore for $ mk $ rounds before choosing a single arm for the rest of the round. ($k$ is the number of actions). The pseudo code is also very simple for this algorithm:</p>
<ol>
<li>Input $ m $</li>
<li>In round $ t $ choose action:</li>
</ol>
<p>$$<br>A_t = \begin{cases} (t mod k) + 1, if \quad t \leq mk; \quad \ argmax_i \hat{\mu_i}(mk), t &gt; mk \end{cases}<br>$$</p>
<h3 id="Regret-Analysis-for-ETC"><a href="#Regret-Analysis-for-ETC" class="headerlink" title="Regret Analysis for ETC:"></a>Regret Analysis for ETC:</h3><p>When ETC algorithm interacting with any 1-sub-gaussian bandit and $ 1 \leq m \leq \frac{n}{k}$, $R_n$ satisfies the following:</p>
<p>$$<br>R_n \leq m \sum_{i=1}^{k}{\Delta_i} + (n-mk)\sum_{i=1}^{k}{\Delta_i exp(-\frac{m(\Delta_i)^2}{4})}<br>$$</p>
<p>From the above inequality, it is easy to see that there is a trade off between exploration and exploitation. If $m$ is large, then the algorithm explores too long, the first term would be very large; if $m$ is too small, the probability that the algorithm commits to the wrong arm will grow, then the second term would be large.</p>
<p>The proof is vert straightforward by using regret decomposition. I will not provide detailed proof here. If you want to read the proof, you can check from the book “Bandit Algorithms”.</p>
<p>Simple code Implementation using Python 3:</p>
<p><img src="/images/code_experiment.png" alt=""></p>
<h2 id="The-Upper-Confidence-Bound-Algorithm-with-Asymptotic-Optimality"><a href="#The-Upper-Confidence-Bound-Algorithm-with-Asymptotic-Optimality" class="headerlink" title="The Upper Confidence Bound Algorithm with Asymptotic Optimality"></a>The Upper Confidence Bound Algorithm with Asymptotic Optimality</h2><h3 id="General-UCB-algorithm"><a href="#General-UCB-algorithm" class="headerlink" title="General UCB algorithm:"></a>General UCB algorithm:</h3><p>UCB algorithm is based on the idea of <strong>optimism in the face of uncertainty</strong>. This term means that one should act as if the environment is as nice as plausibly possible. For bandits, the optimism principle means that using the data learned to assign to each arm with high probability is an overestimate of the unknown mean.</p>
<p>We can further define upper confidence bound by the following:<br>$$<br>\mathbb{P}(\mu \geq \hat{\mu} + \sqrt{\frac{2log(1/\delta)}{n})} \leq \delta \quad \forall \delta \in (0,1)<br>$$</p>
<p>The learner will observe $T_i(t-1)$ samples from arm $i$ and received reward from that arm with an empirical mean of $\hat{\mu_i}(t-1)$</p>
<p>Pseudo code for simple UCB algorithm:</p>
<ol>
<li>Input K and $\delta$(error probability)</li>
<li>for $t \in 1,…,n$ do;</li>
<li>$\quad \quad \quad \quad$Choose action $A_t = argmax_i UCB_i(t-1, \delta)$</li>
<li>$\quad \quad \quad \quad$Observe reward $X_t$ and update upper confidence bounds</li>
<li>end for;</li>
</ol>
<p>Where $UCB_i(t-1, \delta)$is:<br>$$<br>UCB_i(t-1, \delta) = \begin{cases} \infty \quad \quad if \quad T_i(t-1) = 0; \quad \quad \ \hat{\mu_i}(t-1) + \sqrt{\frac{2log(1/\delta)}{T_i(t-1)})} \quad otherwise \end{cases}<br>$$</p>
<p>It is difficult to choose a proper $\delta$ value. Usually, we set $\delta = \frac{1}{N}$ where $N$ is the total rounds at some time $t$. When $ N$ goes to $\infty$, the upper confidence bound goes to 0, which means the reward we observed are close to the true reward. Unfortunately, things are not quite this simple. As we already know that $T_i(t-1)$ is a random variable, and therefore our concentration results cannot be applied directly. For this reason, $\delta$ should be chosen smaller than $\frac{1}{N}$</p>
<h3 id="Regret-Analysis-for-UCB-algorithm"><a href="#Regret-Analysis-for-UCB-algorithm" class="headerlink" title="Regret Analysis for UCB algorithm:"></a>Regret Analysis for UCB algorithm:</h3><p>For any horizon n, if $\delta = \frac{1}{n^2}$, and for stochastic k-armed 1-sub-gaussian bandit problem, we have:<br>$$<br>R_n \leq 3\sum_{i=1}^{k} \Delta_i + \sum_{i:\Delta_i&gt;0}{\frac{16log(n)}{\Delta_i}}<br>$$</p>
<p>As for now, we already talk about general UCB algorithm. From here to the end, we will talk about UCB Algorithm with Asymptotic Optimality.</p>
<h3 id="Asymptotically-Optimal-UCB"><a href="#Asymptotically-Optimal-UCB" class="headerlink" title="Asymptotically Optimal UCB"></a>Asymptotically Optimal UCB</h3><p>The difference between Asymptotically Optimal UCB and general UCB algorithm is Asymptotically Optimal UCB does not depend on error probability($\delta$) and this algorithm tries every arm once and then select for the optimal.</p>
<p>Pseudo code for Asymptotically optimal UCB:</p>
<ol>
<li>Input $k$</li>
<li>Choose each arm once</li>
<li>Subsequently choose:<br>$$<br>A_t = argmax_i(\hat{\mu_i}(t-1) + \sqrt{\frac{2logf(t)}{T_i(t-1)}} )<br>$$<br>Where $f(t) = 1 + t^2log(t)$</li>
</ol>
<h3 id="Regret-Analysis"><a href="#Regret-Analysis" class="headerlink" title="Regret Analysis:"></a>Regret Analysis:</h3><p>For any 1-sub-gaussian bandit, the regret satisfies the following result:<br>$$<br>R_n \leq \sum_{i:\Delta_i&gt;0} \inf_{\varepsilon \in (0, \Delta_i)} \Delta_i (1 + \frac{5}{\varepsilon^2} + \frac{2(logf(n) + \sqrt{\pi logf(n)} + 1)}{(\Delta_i-\varepsilon)^2}) \quad \quad \quad \quad (1)<br>$$<br>Choosing $\varepsilon = \frac{\Delta_i}{2}$ inside the sum, we get:<br>$$<br>R_n \leq \sum_{i:\Delta_i&gt;0} (\Delta_i + \frac{1}{\Delta_i}(8logf(n) + 8\sqrt{\pi logf(n)} + 28))<br>$$</p>
<h3 id="Proof"><a href="#Proof" class="headerlink" title="** Proof **:"></a>** Proof **:</h3><p>I will start with introduce Hoeffding’s Inequality [Check here for details]: <a href="http://cs229.stanford.edu/extra-notes/hoeffding.pdf" target="_blank" rel="noopener">http://cs229.stanford.edu/extra-notes/hoeffding.pdf</a></p>
<p>Hoeffding’s bound: Assume that $X_i - \mu$ are independent, $\sigma^2$-sub-gaussian random variables. Then, their average $\mu$ satisfies the following:<br>$$<br>\mathbb{P}(\hat{\mu} \geq \mu + \varepsilon) \leq exp(-\frac{n\varepsilon^2}{2\sigma^2}) \quad and, \quad \mathbb{P}(\hat{\mu} \leq \mu - \varepsilon) \leq exp(-\frac{n\varepsilon^2}{2\sigma^2})<br>$$</p>
<p>Next, I will introduce <strong>concentration inequality</strong> for sub-gaussian distribution.<br>If $X$ is a $\sigma^2$-sub-gaussian, then $\mathbb{P}(X \geq \varepsilon) \leq exp(-\frac{\varepsilon^2}{2\sigma^2})$</p>
<p>Lemma 1: Let $X_1, X_2,…$ be a sequence of independent 1-sub-gaussian random variables, $\hat{\mu_t} = \sum_{s=1}^{t}{\frac{X_s}{t}}, \varepsilon &gt; 0$ and $\kappa = \sum_{t=1}^{n} \mathbb{I}[\hat{\mu} + \sqrt{\frac{2a}{t}} \geq \varepsilon]$, where $u = \frac{2a}{\varepsilon^2}$<br>Then, $E[\kappa] = 1 + \frac{2}{\varepsilon^2}(a + \sqrt{\pi a} + 1)$</p>
<p>Proof of Lemma 1:<br>By using Hoeffding’s bound we have<br>$$<br>E[\kappa] \leq u + \sum_{t=\lceil u \rceil}^{n}{\mathbb{P}(\hat{\mu_t} + \sqrt{\frac{2a}{t}} \geq \varepsilon)}<br>$$<br>$$<br>\leq u + \sum_{t=\lceil u \rceil}^{n}{exp(- \frac{t(\varepsilon -\sqrt{\frac{2a}{t}})^2}{2})}<br>$$<br>$$<br>\leq 1 + u + \int_{u}^{\infty}{exp(- \frac{t(\varepsilon -\sqrt{\frac{2a}{t}})^2}{2})}dt<br>$$<br>$$<br>= 1 + \frac{2}{\varepsilon^2}(a + \sqrt{\pi a} + 1)<br>$$<br>The final inequality is done by substitution $s = \varepsilon \sqrt{t} - \sqrt{2a}$ and change the value of $u$ to $u = \frac{2a}{\varepsilon^2}$ as it stated in Lemma 1.<br>Thus complete the proof of Lemma 1.</p>
<p>I will start to proof equation (1) in regret analysis:<br>Starting from Regret Decomposition:<br>$$<br>R_n = \sum_{i:\Delta_i&gt;0}{\Delta_i E[T_i(n)]}<br>$$<br>We have to find a bound for $E[T_i(n)]$, let $i$ be a optimal arm($\Delta_i&gt;0$). We can split $E[T_i(n)]$ in to two parts, the first part is the number of times the index of the optimal arm is less than $\mu_1-\varepsilon$. The second part is the number of times that $A_t = i$ and its index is larger than $\mu_1-\varepsilon$. Therefore,<br>$$<br>T_i(n) = \sum_{t=1}^{n} {\mathbb{I}(A_t = i)} \leq \sum_{t=1}^{n}{\mathbb{I}[\hat{\mu_1}(t-1) + \sqrt{\frac{2logf(t)}{T_1(t-1)}}]\leq \mu_1-\varepsilon}<br>\quad +<br>$$<br>$$<br>\sum_{t=1}^{n}{\mathbb{I}[\hat{\mu_i}(t-1) + \sqrt{\frac{2logf(t)}{T_i(t-1)}}]\geq \mu_1-\varepsilon \quad and A_t = i}<br>$$</p>
<p>The first part is done by concentration inequality introduced above:<br>$$<br>E[ \sum_{t=1}^{n}{\mathbb{I}[\hat{\mu_1}(t-1) + \sqrt{\frac{2logf(t)}{T_1(t-1)}}]\leq \mu_1-\varepsilon}] \quad = \quad<br>\sum_{t=1}^{n}{\mathbb{P}(\hat{\mu_1}(t-1) + \sqrt{\frac{2logf(t)}{T_1(t-1)}}\leq \mu_1-\varepsilon)}<br>$$<br>$$<br>\leq \sum_{t=1}^{n}\sum_{s=1}^{n} {\mathbb{P}(\hat{\mu_{1,s}} + \sqrt{\frac{2logf(t)}{s}}\leq \mu_1-\varepsilon}) \quad (by \ def.\ of \ unoin \ bound)<br>$$<br>$$<br>\leq \sum_{t=1}^{n}\sum_{s=1}^{n} {exp(-\frac{s(\sqrt{\frac{2logf(t)}{s}} + \varepsilon)^2}{2})} \quad (concentration \ inequality)<br>$$<br>$$<br>\leq \sum_{t=1}^{n} {\frac{1}{f(t)}} \sum_{s=1}^{n} {exp(-\frac{s\varepsilon^2}{2})} \quad ((a+b)^2 \geq a^2+b^2 \ and \ use \ direct \ algebra)<br>$$<br>Derivation of $\sum_{s=1}^{n} {exp(-\frac{s\varepsilon^2}{2})}$: (2)<br>$$<br>\sum_{s=1}^{n} {exp(-\frac{s\varepsilon^2}{2})} = \frac{exp(-\frac{\varepsilon^2}{2})}{1-exp(-\frac{\varepsilon^2}{2})} \quad (sum \ of\ geometric \ series)<br>$$<br>Let $exp(-\frac{\varepsilon^2}{2}) = a$, Claim that $\frac{exp(-a)}{1-exp(-a)} \leq \frac{1}{a}$<br>Proof of claim:<br>$$<br>\frac{exp(-a)}{1-exp(-a)} = \frac{1}{exp(a)-1} \leq \frac{1}{a} \quad implies \quad 1+a \leq exp(a)<br>$$<br>Let $f(x) = exp(x)-1-x$ and $\frac{df}{dx} = exp(x) - 1$. Set $\frac{df}{dx} = 0 \quad iff \quad x = 0$. Therefore $exp(x)$ is always greater or equal than 1+x. Thus complete the proof.<br>Therefore, back to (2):<br>$$<br>\sum_{s=1}^{n} {exp(-\frac{s\varepsilon^2}{2})} = \frac{exp(-\frac{\varepsilon^2}{2})}{1-exp(-\frac{\varepsilon^2}{2})} \leq \frac{1}{\frac{\varepsilon^2}{2}} = \frac{2}{\varepsilon^2}<br>$$<br>Therefore (2) is bounded by $\frac{2}{\varepsilon^2}$.<br>Derivation of $\frac{1}{f(t)}$(3) using Integral Scaling:<br>$$<br>\sum_{t=1}^{n} {\frac{1}{f(t)}} = \sum_{t=1}^{\infty} {\frac{1}{f(t)}} \leq 1 + \int_{1}^{\infty}{\frac{1}{1+xlog^2(x)}}dx \leq \frac{5}{2}<br>$$<br>Combine (2) and (3), we have the first part is bounded by $\frac{5}{\varepsilon^2}$<br>Now, for second part, we will use Lemma 1 directly:<br>$$<br>E[\sum_{t=1}^{n}{\mathbb{I}[\hat{\mu_i}(t-1) + \sqrt{\frac{2logf(t)}{T_i(t-1)}}]\geq \mu_1-\varepsilon \quad and \ A_t = i}]<br>$$<br>$$<br>\leq E[\sum_{t=1}^{n}{\mathbb{I}[\hat{\mu_i}(t-1) + \sqrt{\frac{2logf(n)}{T_i(t-1)}}]\geq \mu_1-\varepsilon \quad and \ A_t = i}]<br>$$<br>$$<br>\leq E[\sum_{s=1}^{n}{\mathbb{I}(\hat{\mu_{1,s}} + \sqrt{\frac{2logf(n)}{s}}\geq \mu_1-\varepsilon})]<br>$$<br>$$<br>= E[\sum_{s=1}^{n}{\mathbb{I}(\hat{\mu_{1,s}} - \mu_i + \sqrt{\frac{2logf(n)}{s}}\geq \Delta_i-\varepsilon})]<br>$$<br>$$<br>\leq 1 + \frac{2}{(\Delta_i-\varepsilon)^2}(logf(n) + \sqrt{\pi logf(n)} + 1) \quad (By \ Using \ Lemma1)<br>$$</p>
<p>Combining all results, we have:<br>$$<br>R_n \leq \frac{5}{\varepsilon^2} + 1 + \frac{2}{(\Delta_i-\varepsilon)^2}(logf(n) + \sqrt{\pi logf(n)} + 1) =  \sum_{i:\Delta_i&gt;0} \inf_{\varepsilon \in (0, \Delta_i)} \Delta_i (1 + \frac{5}{\varepsilon^2} + \frac{2(logf(n) + \sqrt{\pi logf(n)} + 1)}{(\Delta_i-\varepsilon)^2})<br>$$<br>Thus complete the proof. </p>
<p>If you follow me from the start to here, then congratulations! You now know the general idea, algorithm as well as statistical knowledge behind those algorithms. In the next article, I will be discuss other cases of UCB Algorithm(KL_UCB). See you next time! φ(≧ω≦*)♪</p>
<p>References:</p>
<ol>
<li><p><a href="https://banditalgs.com/" target="_blank" rel="noopener">https://banditalgs.com/</a></p>
</li>
<li><p><a href="https://github.com/niravnb/Multi-armed-bandit-algortihms/tree/master/Explore%20then%20commit" target="_blank" rel="noopener">https://github.com/niravnb/Multi-armed-bandit-algortihms/tree/master/Explore%20then%20commit</a></p>
</li>
<li><p><a href="https://ece.iisc.ac.in/~aditya/E1245_Online_Prediction_Learning_F2018/lattimore-szepesvari18bandit-algorithms.pdf" target="_blank" rel="noopener">https://ece.iisc.ac.in/~aditya/E1245_Online_Prediction_Learning_F2018/lattimore-szepesvari18bandit-algorithms.pdf</a></p>
</li>
</ol>

    </div>

    
    
    
        <div class="reward-container">
  <div>I like drinking tea while writing blogs, if you enjoy my posts or find them helpful, I wouldn't mind for $1-2 donations ╰(*°▽°*)╯</div>
  <button onclick="var qr = document.getElementById('qr'); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    Donate
  </button>
  <div id="qr" style="display: none;">
      
      <div style="display: inline-block;">
        <img src="/images/wechatpay.jpg" alt="Simon Zhou WeChat Pay">
        <p>WeChat Pay</p>
      </div>
      
      <div style="display: inline-block;">
        <img src="/images/paypal.jpg" alt="Simon Zhou PayPal">
        <p>PayPal</p>
      </div>

  </div>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Reinforcement-Learning/" rel="tag"># Reinforcement Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/06/19/A-simple-work-through-of-a-Kaggle-Competition-Project/" rel="prev" title="Kaggle Competition Project - What's Cooking (With Code!)">
      <i class="fa fa-chevron-left"></i> Kaggle Competition Project - What's Cooking (With Code!)
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/07/03/Common-Machine-Learning-Engineer-Interview-Questions/" rel="next" title="Common Machine Learning Engineer Interview Questions">
      Common Machine Learning Engineer Interview Questions <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Reinforcement-Learning-ETC-and-UCB-Algorithm"><span class="nav-number">1.</span> <span class="nav-text">Reinforcement Learning - ETC and UCB Algorithm</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Intuition-of-Exploration-Vs-Exploitation"><span class="nav-number">1.1.</span> <span class="nav-text">Intuition of Exploration Vs. Exploitation</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#What-is-Multi-Armed-Bandit-K-Armed-Bandit"><span class="nav-number">1.2.</span> <span class="nav-text">What is Multi-Armed Bandit (K-Armed Bandit)?</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#What-is-Stochastic-Bandit"><span class="nav-number">1.2.1.</span> <span class="nav-text">What is Stochastic Bandit:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Environment-Classes"><span class="nav-number">1.2.2.</span> <span class="nav-text">Environment Classes:</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Unstructured-Bandits"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">Unstructured Bandits:</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Structured-Bandits"><span class="nav-number">1.2.3.</span> <span class="nav-text">Structured Bandits:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regret-Analysis-Decomposition"><span class="nav-number">1.2.4.</span> <span class="nav-text">Regret Analysis&#x2F;Decomposition:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Explore-Then-Commit-Algorithm-for-Stochastic-Bandit-Problem-with-Finitely-many-Arms"><span class="nav-number">1.3.</span> <span class="nav-text">The Explore-Then-Commit Algorithm for Stochastic Bandit Problem with Finitely many Arms</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ETC-Algorithm"><span class="nav-number">1.3.1.</span> <span class="nav-text">ETC Algorithm:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regret-Analysis-for-ETC"><span class="nav-number">1.3.2.</span> <span class="nav-text">Regret Analysis for ETC:</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Upper-Confidence-Bound-Algorithm-with-Asymptotic-Optimality"><span class="nav-number">1.4.</span> <span class="nav-text">The Upper Confidence Bound Algorithm with Asymptotic Optimality</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#General-UCB-algorithm"><span class="nav-number">1.4.1.</span> <span class="nav-text">General UCB algorithm:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regret-Analysis-for-UCB-algorithm"><span class="nav-number">1.4.2.</span> <span class="nav-text">Regret Analysis for UCB algorithm:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Asymptotically-Optimal-UCB"><span class="nav-number">1.4.3.</span> <span class="nav-text">Asymptotically Optimal UCB</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regret-Analysis"><span class="nav-number">1.4.4.</span> <span class="nav-text">Regret Analysis:</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Proof"><span class="nav-number">1.4.5.</span> <span class="nav-text">** Proof **:</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Simon Zhou"
      src="/images/profilepic.jpg">
  <p class="site-author-name" itemprop="name">Simon Zhou</p>
  <div class="site-description" itemprop="description">Update from time to time. Stay tuned!</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">4</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">3</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/simonZhou86" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;simonZhou86" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:zhoumeng986@gmail.com" title="E-Mail → mailto:zhoumeng986@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/simon-zhou-6021a7169/" title="LinkedIn → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;simon-zhou-6021a7169&#x2F;" rel="noopener" target="_blank"><i class="fab fa-linkedin fa-fw"></i>LinkedIn</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">by Simon Zhou</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script size="300" alpha="0.6" zIndex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'IVDlJzvMaT5PBJv4SbIPrji2-gzGzoHsz',
      appKey     : 'itw2dSro5du78aQjcUJdYOz2',
      placeholder: "Comment goes here",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : true,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : false,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
